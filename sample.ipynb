{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実践問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_dict = pd.read_csv('要因.csv', index_col=0).to_dict()['CAUSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unknown'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>nr</th>\n",
       "      <th>serial</th>\n",
       "      <th>error_code</th>\n",
       "      <th>factor</th>\n",
       "      <th>print_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020/1/1</td>\n",
       "      <td>2020/1/2</td>\n",
       "      <td>500-1</td>\n",
       "      <td>aaa</td>\n",
       "      <td>1</td>\n",
       "      <td>PQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020/1/2</td>\n",
       "      <td>2020/1/7</td>\n",
       "      <td>500-2</td>\n",
       "      <td>aaa</td>\n",
       "      <td>2</td>\n",
       "      <td>PQ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020/1/3</td>\n",
       "      <td>2020/2/7</td>\n",
       "      <td>500-3</td>\n",
       "      <td>aaa</td>\n",
       "      <td>4</td>\n",
       "      <td>JAM</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020/1/4</td>\n",
       "      <td>2020/1/10</td>\n",
       "      <td>600-1</td>\n",
       "      <td>aaa</td>\n",
       "      <td>8</td>\n",
       "      <td>PQ</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020/1/5</td>\n",
       "      <td>2020/2/20</td>\n",
       "      <td>600-1</td>\n",
       "      <td>aaa</td>\n",
       "      <td>3</td>\n",
       "      <td>Noise</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020/1/6</td>\n",
       "      <td>2020/3/20</td>\n",
       "      <td>600-1</td>\n",
       "      <td>aaa</td>\n",
       "      <td>2</td>\n",
       "      <td>Noise</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020/1/7</td>\n",
       "      <td>2020/1/7</td>\n",
       "      <td>701-1</td>\n",
       "      <td>bbb</td>\n",
       "      <td>4</td>\n",
       "      <td>JAM</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020/1/8</td>\n",
       "      <td>2020/1/2</td>\n",
       "      <td>500-1</td>\n",
       "      <td>bbb</td>\n",
       "      <td>8</td>\n",
       "      <td>PQ</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020/1/9</td>\n",
       "      <td>2020/1/7</td>\n",
       "      <td>500-2</td>\n",
       "      <td>bbb</td>\n",
       "      <td>1</td>\n",
       "      <td>JAM</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020/1/10</td>\n",
       "      <td>2020/2/7</td>\n",
       "      <td>500-3</td>\n",
       "      <td>bbb</td>\n",
       "      <td>2</td>\n",
       "      <td>JAM</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020/1/11</td>\n",
       "      <td>2020/1/10</td>\n",
       "      <td>600-1</td>\n",
       "      <td>bbb</td>\n",
       "      <td>7392</td>\n",
       "      <td>PQ</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020/1/12</td>\n",
       "      <td>2020/2/20</td>\n",
       "      <td>600-1</td>\n",
       "      <td>bbb</td>\n",
       "      <td>8</td>\n",
       "      <td>Noise</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020/1/13</td>\n",
       "      <td>2020/3/20</td>\n",
       "      <td>600-1</td>\n",
       "      <td>bbb</td>\n",
       "      <td>1</td>\n",
       "      <td>Noise</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020/1/14</td>\n",
       "      <td>2020/1/7</td>\n",
       "      <td>701-1</td>\n",
       "      <td>bbb</td>\n",
       "      <td>2</td>\n",
       "      <td>JAM</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020/1/15</td>\n",
       "      <td>2020/1/7</td>\n",
       "      <td>801-1</td>\n",
       "      <td>bbb</td>\n",
       "      <td>4</td>\n",
       "      <td>JAM</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020/1/16</td>\n",
       "      <td>2020/1/7</td>\n",
       "      <td>801-1</td>\n",
       "      <td>ccc</td>\n",
       "      <td>8</td>\n",
       "      <td>JAM</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020/1/17</td>\n",
       "      <td>2020/1/7</td>\n",
       "      <td>801-1</td>\n",
       "      <td>ccc</td>\n",
       "      <td>1</td>\n",
       "      <td>JAM</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start        end     nr serial  error_code factor  print_count\n",
       "0    2020/1/1   2020/1/2  500-1    aaa           1     PQ            1\n",
       "1    2020/1/2   2020/1/7  500-2    aaa           2     PQ            2\n",
       "2    2020/1/3   2020/2/7  500-3    aaa           4    JAM            3\n",
       "3    2020/1/4  2020/1/10  600-1    aaa           8     PQ            4\n",
       "4    2020/1/5  2020/2/20  600-1    aaa           3  Noise            5\n",
       "5    2020/1/6  2020/3/20  600-1    aaa           2  Noise            6\n",
       "6    2020/1/7   2020/1/7  701-1    bbb           4    JAM            7\n",
       "7    2020/1/8   2020/1/2  500-1    bbb           8     PQ            8\n",
       "8    2020/1/9   2020/1/7  500-2    bbb           1    JAM            9\n",
       "9   2020/1/10   2020/2/7  500-3    bbb           2    JAM           10\n",
       "10  2020/1/11  2020/1/10  600-1    bbb        7392     PQ           11\n",
       "11  2020/1/12  2020/2/20  600-1    bbb           8  Noise           12\n",
       "12  2020/1/13  2020/3/20  600-1    bbb           1  Noise           13\n",
       "13  2020/1/14   2020/1/7  701-1    bbb           2    JAM           14\n",
       "14  2020/1/15   2020/1/7  801-1    bbb           4    JAM           15\n",
       "15  2020/1/16   2020/1/7  801-1    ccc           8    JAM           16\n",
       "16  2020/1/17   2020/1/7  801-1    ccc           1    JAM           17"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('test1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaa', 'bbb', 'ccc'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deviceid_list = df.serial.unique()\n",
    "deviceid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         start        end     nr serial  error_code factor  print_count\n",
      "0    2020/1/1   2020/1/2  500-1    aaa           1     PQ            1\n",
      "1    2020/1/2   2020/1/7  500-2    aaa           2     PQ            2\n",
      "2    2020/1/3   2020/2/7  500-3    aaa           4    JAM            3\n",
      "3    2020/1/4  2020/1/10  600-1    aaa           8     PQ            4\n",
      "4    2020/1/5  2020/2/20  600-1    aaa           3  Noise            5\n",
      "5    2020/1/6  2020/3/20  600-1    aaa           2  Noise            6\n",
      "6    2020/1/7   2020/1/7  701-1    bbb           4    JAM            7\n",
      "7    2020/1/8   2020/1/2  500-1    bbb           8     PQ            8\n",
      "8    2020/1/9   2020/1/7  500-2    bbb           1    JAM            9\n",
      "9   2020/1/10   2020/2/7  500-3    bbb           2    JAM           10\n",
      "10  2020/1/11  2020/1/10  600-1    bbb        7392     PQ           11\n",
      "11  2020/1/12  2020/2/20  600-1    bbb           8  Noise           12\n",
      "12  2020/1/13  2020/3/20  600-1    bbb           1  Noise           13\n",
      "13  2020/1/14   2020/1/7  701-1    bbb           2    JAM           14\n",
      "14  2020/1/15   2020/1/7  801-1    bbb           4    JAM           15\n",
      "2         start       end     nr serial  error_code factor  print_count\n",
      "15  2020/1/16  2020/1/7  801-1    ccc           8    JAM           16\n",
      "16  2020/1/17  2020/1/7  801-1    ccc           1    JAM           17\n"
     ]
    }
   ],
   "source": [
    "n = round(len(deviceid_list)/2)\n",
    "\n",
    "for i in range(0, len(deviceid_list), n):\n",
    "    device = deviceid_list[i: i+n]\n",
    "    _df = df[df.serial.isin(device)]\n",
    "    print(i,_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3, 2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カテゴリと記事タイトルがnews_titles_and_categories.csvに保存されました。\n"
     ]
    }
   ],
   "source": [
    "# Yahoo!ニュースのトピック一覧ページからタイトルとカテゴリを取得\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_titles_and_categories(url):\n",
    "    # URLにアクセスしてページのHTMLを取得\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # 各カテゴリーとその記事タイトルを抽出\n",
    "    sections = soup.find_all('div', class_='sc-hwj2au-1')\n",
    "    data = []\n",
    "\n",
    "    for section in sections:\n",
    "        category = section.find('a').get_text(strip=True)\n",
    "        articles = section.find_all('li', class_='sc-1nhdoj2-0')\n",
    "        for article in articles:\n",
    "            title = article.find('a').get_text(strip=True)\n",
    "            data.append({\"Category\": category, \"Title\": title})\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Yahoo!ニュースのトピック一覧ページからタイトルとカテゴリを取得\n",
    "url = \"https://news.yahoo.co.jp/topics\"\n",
    "data = get_titles_and_categories(url)\n",
    "\n",
    "# データをデータフレームに変換\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# データフレームをCSVファイルに保存\n",
    "df.to_csv('news_titles_and_categories.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"カテゴリと記事タイトルがnews_titles_and_categories.csvに保存されました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "以下のサイトから、一ヶ月以内に更新された「製品・ソリューション」の情報のみを取得するPythonのコードを教えてください。\n",
    "情報がある場合は、pandasを用いて、csvファイルで出力してください。\n",
    "コメントは日本語で書いてください。\n",
    "https://www.kyoceradocumentsolutions.com/ja/news/rls_2024/\n",
    "\n",
    "一つの記事は、以下のような構成です。\n",
    "この記事の場合は、2024/12/09が更新日で、「製品・ソリューション」の情報です。\n",
    "<li>\n",
    "                        <div class=\"news-entries__date\">2024/12/09</div>\n",
    "                        <div class=\"news-entries__category\">\n",
    "                            <a href=\"/ja/news/products-solutions/\">製品・ソリューション</a>\n",
    "                        </div>\n",
    "                        <div class=\"news-entries__title\"><a href=\"/ja/news/rls_2024/rls_20241209.html\">カラーA4プリンターECOSYS PA2600cwx、カラーA4複合機ECOSYS MA2600cwfxを新発売</a></div>\n",
    "                    </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSVファイルに出力しました\n"
     ]
    }
   ],
   "source": [
    "# ニュースサイトから、一ヶ月以内の記事を取得する\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 基本URLを設定します\n",
    "base_url = \"https://www.kyoceradocumentsolutions.com\"\n",
    "\n",
    "# ニュースのURLを設定します\n",
    "news_url = \"https://www.kyoceradocumentsolutions.com/ja/news/rls_2024/\"\n",
    "\n",
    "# ページを取得します\n",
    "response = requests.get(news_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# 現在の日付を取得します\n",
    "now = datetime.now()\n",
    "one_month_ago = now - timedelta(days=40)\n",
    "\n",
    "# データを格納するリストを準備します\n",
    "data = []\n",
    "\n",
    "# ニュースエントリーをすべて抽出します\n",
    "entries = soup.find_all(\"li\")\n",
    "for entry in entries:\n",
    "    date_div = entry.find(\"div\", class_=\"news-entries__date\")\n",
    "    category_div = entry.find(\"div\", class_=\"news-entries__category\")\n",
    "    title_div = entry.find(\"div\", class_=\"news-entries__title\")\n",
    "    \n",
    "    # 必要な要素がすべて存在するか確認します\n",
    "    if date_div and category_div and title_div:\n",
    "        date_str = date_div.text.strip()\n",
    "        category = category_div.text.strip()\n",
    "        title = title_div.text.strip()\n",
    "        relative_link = title_div.find(\"a\")[\"href\"]\n",
    "        full_link = base_url + relative_link  # フルパスに変換\n",
    "        \n",
    "        # 日付をdatetimeオブジェクトに変換します\n",
    "        entry_date = datetime.strptime(date_str, \"%Y/%m/%d\")\n",
    "        \n",
    "        # カテゴリが「製品・ソリューション」であり、かつ一ヶ月以内に更新された記事のみを抽出します\n",
    "        if category == \"製品・ソリューション\" and entry_date >= one_month_ago:\n",
    "            data.append([entry_date, category, title, full_link])\n",
    "\n",
    "# pandasのDataFrameを作成します\n",
    "df = pd.DataFrame(data, columns=[\"日付\", \"カテゴリ\", \"タイトル\", \"リンク\"])\n",
    "\n",
    "# DataFrameをcsvファイルに出力します\n",
    "df.to_csv(\"products_solutions_updates.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"CSVファイルに出力しました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     講師名      所属企業        生年月日   出身  \\\n",
      "0  今西 航平       NaN         NaN  NaN   \n",
      "1    NaN  株式会社キカガク         NaN  NaN   \n",
      "2    NaN       NaN  1994年7月15日  NaN   \n",
      "3    NaN       NaN         NaN  千葉県   \n",
      "4    NaN       NaN         NaN  NaN   \n",
      "\n",
      "                                                  趣味  \n",
      "0                                                NaN  \n",
      "1                                                NaN  \n",
      "2                                                NaN  \n",
      "3                                                NaN  \n",
      "4  \\n              バスケットボール\\n              読書\\n  ...  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Safari WebDriverを設定します\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# 指定されたURLにアクセスします\n",
    "driver.get(\"https://scraping-for-beginner.herokuapp.com/login_page\")\n",
    "\n",
    "# ユーザ名とパスワードを入力します\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "password = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "username.send_keys(\"imanishi\")  # ここに実際のユーザ名を入力してください\n",
    "password.send_keys(\"kohei\")  # ここに実際のパスワードを入力してください\n",
    "\n",
    "# ログインボタンをクリックします\n",
    "login_button = driver.find_element(By.CLASS_NAME, \"btn\")\n",
    "login_button.click()\n",
    "\n",
    "# ログイン後のページが読み込まれるまで少し待機します\n",
    "time.sleep(5)\n",
    "\n",
    "# 表を取得します\n",
    "table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "\n",
    "# 表の行データを解析して辞書リストに変換します\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    column_name = row.find_element(By.TAG_NAME, \"th\").text\n",
    "    value = cells[0].text if len(cells) > 0 else \"\"\n",
    "    data.append({column_name: value})\n",
    "\n",
    "# DataFrameに変換します\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrameを表示します\n",
    "print(df)\n",
    "\n",
    "# 必要に応じてCSVファイルに保存します\n",
    "df.to_csv(\"mypage_table.csv\", index=False)\n",
    "\n",
    "# ブラウザを閉じます\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "以下のサイトから、一ヶ月以内に更新されたの情報のみを取得するPythonのコードを教えてください。\n",
    "情報がある場合は、pandasを用いて、csvファイルで出力してください。\n",
    "コメントは日本語で書いてください。\n",
    "https://newsroom.lexmark.com/newsreleases\n",
    "\n",
    "一つの記事は、以下のような構成です。\n",
    "この記事の場合は、2025/1/7が更新日で、「Lexmark Named 2025 Leader in LGBTQ+ Workplace Inclusion」の情報です。<li class=\"wd_item\">\n",
    "<div class=\"wd_item_wrapper\">\n",
    "\t<div class=\"wd_date\">Jan 7, 2025</div>\n",
    "\t<div class=\"wd_title\"><a href=\"https://newsroom.lexmark.com/2025-01-07-Lexmark-Named-2025-Leader-in-LGBTQ-Workplace-Inclusion\">Lexmark Named 2025 Leader in LGBTQ+ Workplace Inclusion</a></div>\n",
    "\t\n",
    "\t<div class=\"wd_summary\"><p>Kentucky-based company has been recognized in Human Rights Campaign Foundation's Corporate Equality Index each year since survey began LEXINGTON, Ky., Jan. 7, 2025 /PRNewswire/ -- Lexmark, a...</p></div>\n",
    "\t\n",
    "\t\n",
    "</div>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSVファイルが生成されました。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 現在の日付を取得します\n",
    "current_date = datetime.now()\n",
    "\n",
    "# サイトのURL\n",
    "url = \"https://newsroom.lexmark.com/newsreleases\"\n",
    "\n",
    "# リクエストを送信してHTMLを取得します\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# ニュース記事を取得します\n",
    "articles = soup.find_all(\"li\", class_=\"wd_item\")\n",
    "\n",
    "# データを格納するリスト\n",
    "data = []\n",
    "\n",
    "# 各記事を解析します\n",
    "for article in articles:\n",
    "    date_str = article.find(\"div\", class_=\"wd_date\").text.strip()\n",
    "    date = datetime.strptime(date_str, \"%b %d, %Y\")\n",
    "    title = article.find(\"div\", class_=\"wd_title\").text.strip()\n",
    "    summary = article.find(\"div\", class_=\"wd_summary\").text.strip()\n",
    "    \n",
    "    # 更新日が一ヶ月以内かどうかを確認します\n",
    "    if (current_date - date) <= timedelta(days=30):\n",
    "        data.append({\n",
    "            \"更新日\": date_str,\n",
    "            \"タイトル\": title,\n",
    "            \"概要\": summary\n",
    "        })\n",
    "\n",
    "# データをDataFrameに変換します\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# CSVファイルに保存します\n",
    "df.to_csv(\"recent_articles.csv\", index=False)\n",
    "\n",
    "print(\"CSVファイルが生成されました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "以下の #要件 を満たすPython のコードを作成してください。\n",
    "#前程条件：\n",
    "・特定のフォルダ内に複数のcsvファイルがある\n",
    "\n",
    "#要件：\n",
    "・ライブラリは、tkinterを使用する\n",
    "・任意のキーワードを入力すると、そのキーワードが文字内部に含まれるcsvファイルを検索する\n",
    "・検索フォルダは、pythonファイルと同じフォルダ内とする\n",
    "・検索する文字列は、部分一致でも可として、大文字と小文字は区別しない\n",
    "・複数の検索結果があるときには、すべて表示する\n",
    "・検索された「csvファイル名」と「検索されたセル内の文字列全体」を表形式で表示する\n",
    "・表示されたファイル名をクリックした場合、csvファイルが開くことができる\n",
    "・検索のステータスバーを表示する\n",
    "・GUIのデフォルトサイズは800×800とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from tkinter import ttk\n",
    "\n",
    "# キーワードでCSVファイルを検索する関数\n",
    "def search_csv_files(keyword):\n",
    "    results = []\n",
    "    keyword = keyword.lower()\n",
    "    for file in os.listdir('.'):\n",
    "        if file.endswith('.csv'):\n",
    "            with open(file, newline='', encoding='utf-8') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                for row in reader:\n",
    "                    for cell in row:\n",
    "                        if keyword in cell.lower():\n",
    "                            results.append((file, cell))\n",
    "                            break\n",
    "    return results\n",
    "\n",
    "# 検索ボタンがクリックされたときに呼び出される関数\n",
    "def on_search():\n",
    "    keyword = keyword_entry.get()\n",
    "    if not keyword:\n",
    "        messagebox.showwarning(\"入力エラー\", \"キーワードを入力してください\")\n",
    "        return\n",
    "\n",
    "    status_label.config(text=\"検索中...\")\n",
    "    root.update_idletasks()\n",
    "\n",
    "    results = search_csv_files(keyword)\n",
    "    for row in tree.get_children():\n",
    "        tree.delete(row)\n",
    "\n",
    "    if results:\n",
    "        for file, cell in results:\n",
    "            tree.insert(\"\", \"end\", values=(file, cell))\n",
    "    else:\n",
    "        messagebox.showinfo(\"結果なし\", \"該当するCSVファイルが見つかりませんでした\")\n",
    "\n",
    "    status_label.config(text=\"検索完了\")\n",
    "\n",
    "# CSVファイルを開く関数\n",
    "def open_csv_file(event):\n",
    "    item = tree.identify('item', event.x, event.y)\n",
    "    if not item:\n",
    "        return\n",
    "\n",
    "    file = tree.item(item, 'values')[0]\n",
    "    os.startfile(file)\n",
    "\n",
    "# GUIのセットアップ\n",
    "root = tk.Tk()\n",
    "root.title(\"CSVファイル検索\")\n",
    "root.geometry(\"800x800\")\n",
    "\n",
    "frame = ttk.Frame(root)\n",
    "frame.pack(padx=10, pady=10, fill=\"x\", expand=True)\n",
    "\n",
    "keyword_label = ttk.Label(frame, text=\"キーワード:\")\n",
    "keyword_label.pack(side=\"left\", padx=(0, 10))\n",
    "\n",
    "keyword_entry = ttk.Entry(frame)\n",
    "keyword_entry.pack(side=\"left\", fill=\"x\", expand=True)\n",
    "\n",
    "search_button = ttk.Button(frame, text=\"検索\", command=on_search)\n",
    "search_button.pack(side=\"left\", padx=(10, 0))\n",
    "\n",
    "tree = ttk.Treeview(root, columns=(\"ファイル名\", \"内容\"), show=\"headings\")\n",
    "tree.heading(\"ファイル名\", text=\"ファイル名\")\n",
    "tree.heading(\"内容\", text=\"内容\")\n",
    "tree.pack(padx=10, pady=10, fill=\"both\", expand=True)\n",
    "\n",
    "tree.bind(\"<Double-1>\", open_csv_file)\n",
    "\n",
    "status_label = ttk.Label(root, text=\"ステータスバー\")\n",
    "status_label.pack(side=\"bottom\", fill=\"x\")\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タイトル: 最新のタブレットが登場しました => カテゴリ: 新製品情報\n",
      "タイトル: 会社の新しいプロジェクトが始動しました => カテゴリ: 会社の重要情報\n",
      "タイトル: 次世代のスマートフォンが発表されました => カテゴリ: 新製品情報\n",
      "タイトル: 新しいCOOが選任されました => カテゴリ: 新製品情報\n",
      "タイトル: 音声をリアルタイムで表示する字幕表示システム「CotopatⓇ Mobile」バージョンアップ版を京セラドキュメントソリューションズジャパンより販売開始 => カテゴリ: 新製品情報\n",
      "タイトル: カラーA4プリンターECOSYS PA2600cwx、カラーA4複合機ECOSYS MA2600cwfxを新発売 => カテゴリ: 新製品情報\n",
      "タイトル: 京セラ株式会社 2025年3月期 第3四半期 決算発表 => カテゴリ: 会社の重要情報\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# spaCyの英語と日本語モデルをロード\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_ja = spacy.load(\"ja_core_news_sm\")\n",
    "\n",
    "# ニュースのタイトルとカテゴリのサンプルデータ\n",
    "titles = [\n",
    "    \"最新のスマートウォッチが市場に投入されました\",\n",
    "    \"企業の業績が大幅に向上しました\",\n",
    "    \"革新的なタブレットが登場\",\n",
    "    \"新しいCEOが就任しました\",\n",
    "    \"次世代のゲームコンソールが発売されました\",\n",
    "    \"会社の株価が急上昇しました\",\n",
    "    \"新しいノートパソコンが発表されました\",\n",
    "    \"企業の新しい戦略が公開されました\",\n",
    "    \"新しいスマートフォンアプリがリリースされました\",\n",
    "    \"取締役会のメンバーが変更されました\",\n",
    "    \"最新のワイヤレスイヤホンが販売開始されました\",\n",
    "    \"大規模なリストラが発表されました\",\n",
    "    \"新しい自動車モデルが登場しました\",\n",
    "    \"会社の新しい本社ビルが完成しました\",\n",
    "    \"新しいスマートホームデバイスが市場に投入されました\",\n",
    "    \"企業の年間収益が記録的な数字に達しました\"\n",
    "]\n",
    "categories = [\n",
    "    \"新製品情報\", \"会社の重要情報\", \"新製品情報\", \"会社の重要情報\",\n",
    "    \"新製品情報\", \"会社の重要情報\", \"新製品情報\", \"会社の重要情報\",\n",
    "    \"新製品情報\", \"会社の重要情報\", \"新製品情報\", \"会社の重要情報\",\n",
    "    \"新製品情報\", \"会社の重要情報\", \"新製品情報\", \"会社の重要情報\"\n",
    "]\n",
    "\n",
    "# テキストの前処理関数\n",
    "def preprocess_text(text):\n",
    "    doc_ja = nlp_ja(text)\n",
    "    doc_en = nlp_en(text)\n",
    "    tokens_ja = [token.lemma_ for token in doc_ja if not token.is_stop]\n",
    "    tokens_en = [token.lemma_ for token in doc_en if not token.is_stop]\n",
    "    return \" \".join(tokens_ja + tokens_en)\n",
    "\n",
    "# 前処理済みのテキストを取得\n",
    "preprocessed_titles = [preprocess_text(title) for title in titles]\n",
    "\n",
    "# ロジスティック回帰とランダムフォレストを使ったアンサンブルモデルの作成\n",
    "model = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(max_iter=1000)),\n",
    "            ('rf', LogisticRegression(max_iter=1000))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    ")\n",
    "\n",
    "# モデルの訓練\n",
    "model.fit(preprocessed_titles, categories)\n",
    "\n",
    "# 新しいニュースタイトルの分類\n",
    "new_titles = [\n",
    "    \"最新のタブレットが登場しました\",\n",
    "    \"会社の新しいプロジェクトが始動しました\",\n",
    "    \"次世代のスマートフォンが発表されました\",\n",
    "    \"新しいCOOが選任されました\",\n",
    "    \"音声をリアルタイムで表示する字幕表示システム「CotopatⓇ Mobile」バージョンアップ版を京セラドキュメントソリューションズジャパンより販売開始\",\n",
    "    \"カラーA4プリンターECOSYS PA2600cwx、カラーA4複合機ECOSYS MA2600cwfxを新発売\",\n",
    "    \"京セラ株式会社 2025年3月期 第3四半期 決算発表\"\n",
    "]\n",
    "preprocessed_new_titles = [preprocess_text(title) for title in new_titles]\n",
    "predicted_categories = model.predict(preprocessed_new_titles)\n",
    "\n",
    "# 分類結果の表示\n",
    "for title, category in zip(new_titles, predicted_categories):\n",
    "    print(f\"タイトル: {title} => カテゴリ: {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport pandas as pd\n",
    "import os\n",
    "\n",
    "# フォルダ内のCSVファイルを処理する関数\n",
    "def process_csv_files(folder_path):\n",
    "    df_list = []  # データフレームを保存するリスト\n",
    "\n",
    "    # フォルダ内のすべてのCSVファイルを取得\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # CSVを読み込んで処理\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.rename(columns={'Unnamed: 0': 'model'})\n",
    "            df = df.set_index('model')\n",
    "            df_list.append(df)\n",
    "    \n",
    "    # 全てのデータフレームを連結\n",
    "    df_merge = pd.concat(df_list,axis=1)\n",
    "    return df_merge\n",
    "\n",
    "# フォルダのパスを指定して関数を実行\n",
    "folder_path = 'test'  # フォルダのパスを入力\n",
    "result_df = process_csv_files(folder_path)\n",
    "result_df = result_df.T\n",
    "\n",
    "# 結果を確認\n",
    "print(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
